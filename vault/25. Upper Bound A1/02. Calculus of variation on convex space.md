

The sofa area functional $\mathcal{A}(K)$ with respect to cap $K \in \mathcal{K}_\omega$ is very likely not a quadratic functional. But we will soon construct upper bounds $\mathcal{A}_1$ and $\mathcal{A}_2$ of $\mathcal{A}$ that are concave quadratic functionals on cap space $\mathcal{K}_\omega$. Then finding the global maximum of $\mathcal{A}_1$ and $\mathcal{A}_2$ on a convex subset of $\mathcal{K}$ can be done using the calculus of variation as the following. Suppose that we want to find the maximizer $K$ of a concave quadratic functional $f(K)$ on a convex space $\mathcal{K}$. Then in particular, for any value $K' \in \mathcal{K}$ the value $f(c_\lambda(K, K'))$ should always attain its maximum at $\lambda = 0$. So the following value should be nonpositive.

> __Definition [convex-space-directional-derivative].__ For any convex space $\mathcal{K}$ with convex combination $c_\lambda(-, -)$ and a quadratic functional $f : \mathcal{K} \to \mathbb{R}$ and $K \in \mathcal{K}$, define the following. ^def-convex-space-directional-derivative
$$
Df(K; K') = \left. \frac{d}{d \lambda} \right|_{\lambda = 0} f(c_\lambda(K, K'))
$$

[[02. Calculus of variation on convex space#^def-convex-space-directional-derivative]] can be seen as a generalization of the Gateaux derivative to functionals on a general convex space. For any quadratic functional $f$ and a fixed $K \in \mathcal{K}$ the value $Df(K; K')$ is always a linear functional of $K'$.

> __Lemma [derivative-calculation].__ Let $f$ be a quadratic functional on a convex space $\mathcal{K}$, so that $f(K) = h(K, K)$ for a convex-bilinear map $h : \mathcal{K} \times \mathcal{K} \to \mathbb{R}$. Then we have the following for any $K, K' \in \mathcal{K}$. ^lem-derivative-calculation
$$
Df(K; K') = h(K, K') + h(K', K) - 2 h (K, K)
$$
> So in particular, the map $Df(K; -) : \mathcal{K} \to \mathbb{R}$ is always well-defined and a linear functional. 

_Proof._ We have the following computation by bilinearlity of $h$. ^eqn-quadratic-functional
$$
\begin{align*}
f(c_\lambda(K, K')) & = h(c_\lambda(K, K'), c_\lambda(K, K')) \\
& = (1 - \lambda)^2 h(K, K') + \lambda (1 - \lambda) \left( h(K, K') + c_\lambda (K', K) \right) + \lambda^2 h(K', K')
\end{align*}
$$
Now take derivative at $\lambda = 0$. □

To prove that $K$ maximizes a concave quadratic functional $f(K)$ on $\mathcal{K}$, we only need to prove that $Df(K; -)$ is a nonpositive linear functional on $\mathcal{K}$. 

> __Theorem [quadratic-variation].__ For any concave quadratic functional $f$ on a convex space $\mathcal{K}$ with convex combination $c_\lambda(-, -)$, the value $K \in \mathcal{K}$ maximizes $f(K)$ if and only if the linear functional $Df(K; -)$ is nonpositive. ^thm-quadratic-variation

_Proof._ The 'only if' part is obvious. Assume an arbitrary $K \in \mathcal{K}$ such that $Df(K; -)$ is always nonpositive. Take any $K' \in \mathcal{K}$. Observe that $f(c_\lambda(K, K'))$ is a polynomial $p(\lambda)$ of $\lambda \in [0, 1]$ by [[02. Calculus of variation on convex space#^eqn-quadratic-functional]]. Because $f$ is concave, the polynomial $p(\lambda)$ is also concave with respect to $\lambda$ and the quadratic coefficient of $p(\lambda)$ is nonpositive. The linear coefficient of $p(\lambda)$ is $Df(K; K')$ which is nonpositive as well. So $p(\lambda)$ is monotonically decreasing with respect to $\lambda$ and we have $f(K) \geq f(K')$ as desired. □

For the rest of this paper, we will maximize $f = \mathcal{A}_1$ or $\mathcal{A}_2$ by essentially solving the equation $Df(K_{\max}; -) = 0$ for $K_{\max}$; while we technically need to solve for the inequality $Df(K_{\text{m}}; -) \geq 0$, modulo minor details it turns out that we are essentially solving for the equality condition. Then the maximum value $\mathcal{A}_1(\mathcal{K}_{\max, 1})$ and $\mathcal{A}_2(\mathcal{K}_{\max, 2})$ will immediately give an upper bound of the sofa area functional $\mathcal{A}$. In fact, the maximum value $\mathcal{A}_1(\mathcal{K}_{\max, 1}) = 1 + \pi^2/8 = 2.2337\dots$ of $\mathcal{A}_1$ turns out to be very close to the area $2.2195\dots$ of Gerver's sofa. The first upper bound $\mathcal{A}_1$ is used to construct a finer bound $\mathcal{A}_2$, and then we will show that the maximizer of $\mathcal{A}_2$ is Gerver's sofa.