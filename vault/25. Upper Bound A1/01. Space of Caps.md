In [[15. Sofa Area Functional#^con-gerver-cap]], we turned the moving sofa problem into optimizing the sofa area functional $\mathcal{A} : \mathcal{K}_\omega \to \mathbb{R}$ on the space of caps $\mathcal{K}_\omega$. Our main observation here is that the space of caps $\mathcal{K}_\omega$ is a _convex space_. A set $\mathcal{K}$ is a convex space if it is equipped with a convex combination operation $c_\lambda(-, -) : \mathcal{K} \times \mathcal{K} \to \mathcal{K}$ for every $\lambda \in [0, 1]$ such that it satisfies a set of axioms matchiing the properties of convex-linear combination in a real vector space (e.g. Axioms 1-4 of [[@nlab-convex-space]]). Any real vector space $V$ is a convex space with $c_\lambda(v_1, v_2) = (1 - \lambda) v_1 + \lambda v_2$. For the space of caps $\mathcal{K}_\Theta$, define their convex combination as $c_\lambda(K_1, K_2) = (1-\lambda) K_1 + \lambda K_2$ using the Minkowski sum. Then it is rountine to check that the combination is also a cap with rotation angle $\omega$, and that the combination satisfies the axioms of convex space.

# Calculus of Variation

Many values on the cap $K \in \mathcal{K}_\omega$ are convex-linear. 

> __Definition [convex-linear].__ A function $f : \mathcal{K} \to V$ from a convex space $\mathcal{K}$ to a convex space $V$ is _convex-linear_ if $f(c_\lambda(K_1, K_2)) = c_\lambda(f(K_1), f(K_2))$ for all $K_1, K_2 \in \mathcal{K}$ and $\lambda \in [0, 1]$. Call a functional $f : \mathcal{K} \to \mathbb{R}$ on $\mathcal{K}$ a _linear functional_ on $\mathcal{K}$ if it is convex-linear. ^def-convex-linear

The support function $p_K$ is convex-linear with respect to $K \in \mathcal{K}_\omega$. For an arbitrary angle $t \in S^1$, the vertices $A_K^+(t), A_K^-(t), C_K^+(t), C_K^-(t)$ ([[15. Sofa Area Functional#^def-cap-vertices]]) are all convex-linear with respect to $K \in \mathcal{K}_\omega$. Likewise, the corners $\mathbf{x}_K(t)$ and $\mathbf{y}_K(t)$ are convex-linear with respect to $K \in \mathcal{K}_\omega$. 

In this paper, for a convex space $\mathcal{K}$ call a function $g : \mathcal{K} \times \mathcal{K} \to \mathbb{R}$ _convex-bilinear_ if the maps $K \mapsto g(K_1, K)$ and $K \mapsto g(K, K_2)$ are convex-linear for any $K_1, K_2 \in \mathcal{K}$. Call a functional $h : \mathcal{K} \to \mathbb{R}$ a _quadratic functional_ on $\mathcal{K}$ if $h(K) = g(K, K)$ for some convex-bilinear function $g$. Note that in our definitions, a linear functional $f$ on $\mathcal{K}$ is also quadratic because $f(K) = g(K, K)$ where $g(K_1, K_2) = f(K_1)$ is convex-bilinear. We will soon see that the area $|K|$ of a cap $K \in \mathcal{K}_\omega$ is a quadratic functional on $\mathcal{K}_\omega$.

The sofa area functional $\mathcal{A}(K)$ with respect to cap $K \in \mathcal{K}_\omega$ is very likely not a quadratic functional. But we will soon construct upper bounds $\mathcal{A}_1$ and $\mathcal{A}_2$ of $\mathcal{A}$ that are concave quadratic functionals on cap space $\mathcal{K}_\omega$. Then finding the global maximum of $\mathcal{A}_1$ and $\mathcal{A}_2$ on a convex subset of $\mathcal{K}$ can be done using the calculus of variation as the following. Suppose that we want to find the maximizer $K$ of a concave quadratic functional $f(K)$ on a convex space $\mathcal{K}$. Then in particular, for any value $K' \in \mathcal{K}$ the value $f(c_\lambda(K, K'))$ should always attain its maximum at $\lambda = 0$. So the following value should be nonpositive.

> __Definition [convex-space-directional-derivative].__ For any convex space $\mathcal{K}$ with convex combination $c_\lambda(-, -)$ and a quadratic functional $f : \mathcal{K} \to \mathbb{R}$ and $K \in \mathcal{K}$, define the following. ^def-convex-space-directional-derivative
$$
Df(K; K') = \left. \frac{d}{d \lambda} \right|_{\lambda = 0} f(c_\lambda(K, K'))
$$

[[01. Space of Caps#^def-convex-space-directional-derivative]] can be seen as a generalization of the Gateaux derivative to functionals on a general convex space. For any quadratic functional $f$ and a fixed $K \in \mathcal{K}$ the value $Df(K; K')$ is always a linear functional of $K'$.

> __Lemma [derivative-calculation].__ Let $f$ be a quadratic functional on a convex space $\mathcal{K}$, so that $f(K) = h(K, K)$ for a convex-bilinear map $h : \mathcal{K} \times \mathcal{K} \to \mathbb{R}$. Then we have the following for any $K, K' \in \mathcal{K}$. ^lem-derivative-calculation
$$
Df(K; K') = h(K, K') + h(K', K) - 2 h (K, K)
$$
> So in particular, the map $Df(K; -) : \mathcal{K} \to \mathbb{R}$ is always well-defined and a linear functional. 

_Proof._ We have the following computation by bilinearlity of $h$. ^eqn-quadratic-functional
$$
\begin{align*}
f(c_\lambda(K, K')) & = h(c_\lambda(K, K'), c_\lambda(K, K')) \\
& = (1 - \lambda)^2 h(K, K') + \lambda (1 - \lambda) \left( h(K, K') + c_\lambda (K', K) \right) + \lambda^2 h(K', K')
\end{align*}
$$
Now take derivative at $\lambda = 0$. □

To prove that $K$ maximizes a concave quadratic functional $f(K)$ on $\mathcal{K}$, we only need to prove that $Df(K; -)$ is a nonpositive linear functional on $\mathcal{K}$. 

> __Theorem [quadratic-variation].__ For any concave quadratic functional $f$ on a convex space $\mathcal{K}$ with convex combination $c_\lambda(-, -)$, the value $K \in \mathcal{K}$ maximizes $f(K)$ if and only if the linear functional $Df(K; -)$ is nonpositive. ^thm-quadratic-variation

_Proof._ The 'only if' part is obvious. Assume an arbitrary $K \in \mathcal{K}$ such that $Df(K; -)$ is always nonpositive. Take any $K' \in \mathcal{K}$. Observe that $f(c_\lambda(K, K'))$ is a polynomial $p(\lambda)$ of $\lambda \in [0, 1]$ by [[01. Space of Caps#^eqn-quadratic-functional]]. Because $f$ is concave, the polynomial $p(\lambda)$ is also concave with respect to $\lambda$ and the quadratic coefficient of $p(\lambda)$ is nonpositive. The linear coefficient of $p(\lambda)$ is $Df(K; K')$ which is nonpositive as well. So $p(\lambda)$ is monotonically decreasing with respect to $\lambda$ and we have $f(K) \geq f(K')$ as desired. □

For the rest of this paper, we will maximize $f = \mathcal{A}_1$ or $\mathcal{A}_2$ by essentially solving the equation $Df(K_{\max}; -) = 0$ for $K_{\max}$; while we technically need to solve for the inequality $Df(K_{\text{m}}; -) \geq 0$, modulo minor details it turns out that we are essentially solving for the equality condition. Then the maximum value $\mathcal{A}_1(\mathcal{K}_{\max, 1})$ and $\mathcal{A}_2(\mathcal{K}_{\max, 2})$ will immediately give an upper bound of the sofa area functional $\mathcal{A}$. In fact, the maximum value $\mathcal{A}_1(\mathcal{K}_{\max, 1}) = 1 + \pi^2/8 = 2.2337\dots$ of $\mathcal{A}_1$ turns out to be very close to the area $2.2195\dots$ of Gerver's sofa. The first upper bound $\mathcal{A}_1$ is used to construct a finer bound $\mathcal{A}_2$, and then we will show that the maximizer of $\mathcal{A}_2$ is Gerver's sofa.