The plan is to reduce the moving sofa problem to the maximization of a concave quadratic functional on a convex space. While there is a formal definition of a _convex space_ (see [@nlab-convex-space]), when we say that we essentially mean a convex subset of a vector space. Recall that any _cancellative convex space_ is indeed a convex subset of a vector space.

> __Theorem [convex-spaces].__ A _cancellative convex space_ $\mathcal{V}$ is a space with the baricentryc operation $c_\lambda : \mathcal{V} \times \mathcal{V} \to \mathcal{V}$ for all $\lambda \in [0, 1]$, such that there is a injection $i : \mathcal{V} \to V$ to a vector space $V$ such that $i(c_{\lambda}(v_1, v_2)) = (1 - \lambda) i(v_1) + \lambda i(v_2)$. ^thm-convex-spaces

> __Definition [convex-spaces].__ All convex spaces we will define here will be cancellative. ^def-convex-spaces

> __Theorem [convex-spaces].__ The space $\mathcal{K}, \mathcal{K}^\mathrm{c}$ of all planar convex bodies, and caps of rotation angle $\pi/2$, are respectively convex spaces with the barycentric operation ^thm-convex-spaces
$$
\begin{align*}
c_\lambda(K_1, K_2) & := (1 - \lambda)K_1 + \lambda K_2 \\
& = \left\{ (1 - \lambda) p_1 + \lambda p_2 : p_1 \in K_1, p_2 \in K_2 \right\} 
\end{align*}
$$
> for all $\lambda \in [0, 1]$, given by the Minkowski sum of convex bodies.

_Proof._  □

> __Definition [convex-linear].__ A function $f : \mathcal{V} \to V$ from a convex space $\mathcal{V}$ to a convex space $V$ is _convex-linear_ if $f(c_\lambda(K_1, K_2)) = c_\lambda(f(K_1), f(K_2))$ for all $K_1, K_2 \in \mathcal{V}$ and $\lambda \in [0, 1]$. Call a functional $f : \mathcal{V} \to \mathbb{R}$ on $\mathcal{V}$ a _linear functional_ on $\mathcal{V}$ if it is convex-linear. ^def-convex-linear

> __Definition [convex-bilinear].__ For convex spaces $\mathcal{V}$ and $V$, call a function $g : \mathcal{V} \times \mathcal{V} \to V$ _convex-bilinear_ if the maps $K \mapsto g(K_1, K)$ and $K \mapsto g(K, K_2)$ are convex-linear for any fixed $K_1, K_2 \in \mathcal{V}$. ^def-convex-bilinear

> __Definition [convex-space-quadratic].__ Call $h : \mathcal{V} \to \mathbb{R}$ a _quadratic functional_ on a convex space $\mathcal{V}$, if $h(K) = g(K, K)$ for some convex-bilinear function $g : \mathcal{V} \times \mathcal{V} \to \mathbb{R}$. ^def-convex-space-quadratic

The _directional derivative_ of a quadratic functional $f$ on a convex space $\mathcal{V}$ is the key for executing the calculus of variation on $f$.

> __Definition [convex-space-directional-derivative].__ For any quadratic functional $f : \mathcal{V} \to \mathbb{R}$ on a convex space $\mathcal{V}$ and $K, K' \in \mathcal{V}$, define the _directional derivative_ ^def-convex-space-directional-derivative
$$
Df(K; K') := \left. \frac{d}{d \lambda} \right|_{\lambda = 0} f(c_\lambda(K, K'))
$$
> of $f$ at $K$ in the direction towards $K'$.

[[old/xb. Upper Bound A1/25. Upper Bound A1/02. Calculus of variation#^def-convex-space-directional-derivative]] can be seen as a generalization of the Gateaux derivative to functionals on a general convex space. For any quadratic functional $f$ and a fixed $K \in \mathcal{V}$, the value $Df(K; K')$ is well-defined and always a linear functional of $K'$.

> __Lemma [derivative-calculation].__ Let $f$ be a quadratic functional on a convex space $\mathcal{V}$, so that $f(K) = h(K, K)$ for a convex-bilinear map $h : \mathcal{V} \times \mathcal{V} \to \mathbb{R}$. Then we have the following for any $K, K' \in \mathcal{V}$. ^lem-derivative-calculation
$$
Df(K; K') = h(K, K') + h(K', K) - 2 h (K, K)
$$
> So in particular, the map $Df(K; -) : \mathcal{V} \to \mathbb{R}$ is always well-defined and a linear functional. 

_Proof._ We have ^eqn-quadratic-functional
$$
\begin{split}
f(c_\lambda(K, K')) & = h(c_\lambda(K, K'), c_\lambda(K, K')) \\
& = (1 - \lambda)^2 h(K, K) + \lambda (1 - \lambda) \left( h(K, K') + h (K', K) \right) + \lambda^2 h(K', K')
\end{split}
$$
by bilinearity of $h$. Now take the derivative at $\lambda = 0$. □

> __Definition [convex-space-concavity].__ A functional $f : \mathcal{V} \to \mathbb{R}$ on a convex space $\mathcal{V}$ is _concave_ (resp. _convex_) if $f(c_\lambda(K_1, K_2)) \geq (1 - \lambda) f(K_1) + \lambda f(K_2)$ (resp. $f(c_\lambda(K_1, K_2)) \leq (1 - \lambda) f(K_1) + \lambda f(K_2)$) for all $K_1, K_2 \in \mathcal{V}$ and $\lambda \in [0, 1]$. ^def-convex-space-concavity

To prove that $K$ maximizes a concave quadratic functional $f(K)$ on $\mathcal{V}$, we only need to prove that $Df(K; -)$ is a non-positive linear functional on $\mathcal{V}$. 

> __Theorem [quadratic-variation].__ For any concave quadratic functional $f$ on a convex space $\mathcal{V}$, the value $K \in \mathcal{V}$ maximizes $f(K)$ if and only if the linear functional $Df(K; -)$ is nonpositive. ^thm-quadratic-variation

_Proof._ Assume that $K$ is the maximizer of $f(K)$. Then for any $K' \in \mathcal{V}$, the value $f(c_\lambda(K, K'))$ over all $\lambda \in [0, 1]$ is maximized at $\lambda = 0$. So taken derivative at $\lambda = 0$, we should have $Df(K; K') \leq 0$.

Now assume on the other hand that $K \in \mathcal{V}$ is chosen such that $Df(K; -)$ is always nonpositive. Take an arbitrary $K' \in \mathcal{V}$ and fix it. Observe that $f(c_\lambda(K, K'))$ is a polynomial $p(\lambda)$ of $\lambda \in [0, 1]$ by [[old/xb. Upper Bound A1/25. Upper Bound A1/02. Calculus of variation#^eqn-quadratic-functional]]. Because $f$ is concave, the polynomial $p(\lambda)$ is also concave with respect to $\lambda$ and the quadratic coefficient of $p(\lambda)$ is nonpositive. The linear coefficient of $p(\lambda)$ is $Df(K; K')$ which is nonpositive as well. So $p(\lambda)$ is monotonically decreasing with respect to $\lambda$ and we have $f(K) \geq f(K')$ as desired. □

> __Lemma [sum-of-squares].__ Let $h : \mathcal{V} \to V$ be a convex-linear map from a convex space $\mathcal{V}$ to a real vector space $V$ with inner product $\left< -, - \right>_V$. Then the quadratic form $f$ on $\mathcal{V}$ defined as $f(K) = \left< h(K), h(K) \right>_V$ is convex. ^lem-sum-of-squares

_Proof._ Take arbitrary $K_1, K_2 \in \mathcal{V}$. Fixing $K_1$ and $K_2$, observe that $f(c_\lambda(K_1, K_2))$ is a quadratic polynomial of $\lambda \in [0, 1]$ with the leading coefficient $\left\lVert h(K_2) - h(K_1) \right\rVert_V^2$ of $\lambda^2$, by expanding the term $h(c_\lambda(K_1, K_2)) = h(K_1) + \lambda (h(K_2) - h(K_1))$ with respect to the inner product $\left< -, - \right>_V$. This shows the convexity of $f$ along the line segment connecting $K_1$ and $K_2$. Since $K_1$ and $K_2$ are chosen arbitrarily, $f$ is convex. □